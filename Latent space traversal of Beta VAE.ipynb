{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "import torch  # Import PyTorch library\n",
    "import torch.nn as nn  # Import neural network module\n",
    "import torch.optim as optim  # Import optimization module\n",
    "from torchvision import datasets, transforms  # Import datasets and transforms\n",
    "from torchvision.utils import save_image, make_grid  # Import utility to save images\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision  # Import torchvision library\n",
    "import matplotlib.pyplot as plt  # Import plotting library\n",
    "import os  # Import os module for file operations\n",
    "import numpy as np  # Import numpy library        nn.InstanceNorm2d(out_channels),\n",
    "\n",
    "from torchinfo import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchvision.models import inception_v3\n",
    "import numpy as np\n",
    "import shutil\n",
    "from PIL import Image  # Import PIL for image processing\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set device\n",
    "print(f\"Using device: {device}\")  # Print the device being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "LATENT_DIM = 128\n",
    "# Hyperparameters for training\n",
    "NUM_EPOCHS = 250\n",
    "tag = os.getenv(\"TAG\", None)\n",
    "tag = f\"_{tag}\" if tag else \"\"\n",
    "\n",
    "TRAIN_VAE = True\n",
    "TRAIN_BETA_VAE = False\n",
    "LOAD_BETA_VAE = False\n",
    "TRAIN_CNN = False\n",
    "TRAIN_MLP = False\n",
    "\n",
    "BUTTERFLY = \"butterfly\"\n",
    "ANIMAL = \"animal\"\n",
    "dataset = os.getenv('DATASET', BUTTERFLY)  # butterfly or animal\n",
    "NUM_SAMPLES = int(os.getenv('NS', 10))  # Number of latent samples per input\n",
    "\n",
    "BATCH_SIZE = 128 if NUM_SAMPLES <= 10 else (96 if NUM_SAMPLES <= 15 else 64)\n",
    "\n",
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = log_dir = f'VAE/tensorboard/{dataset}_ns_{NUM_SAMPLES}{tag}'\n",
    "IMG_DIR = f'VAE/generated_images/{dataset}_ns_{NUM_SAMPLES}{tag}'\n",
    "\n",
    "\n",
    "def recreate_directory(dir_path):\n",
    "    # Check if the directory exists\n",
    "    if os.path.exists(dir_path):\n",
    "        # Delete the directory if it exists\n",
    "        shutil.rmtree(dir_path)\n",
    "    # Create the directory\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "os.makedirs('VAE/models', exist_ok=True)\n",
    "\n",
    "if TRAIN_VAE:\n",
    "    recreate_directory(LOG_DIR)\n",
    "    recreate_directory(IMG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tensorboard stuff\n",
    "writer = SummaryWriter(LOG_DIR)\n",
    "event_acc = EventAccumulator(LOG_DIR)\n",
    "\n",
    "\n",
    "def log_losses_to_tensorboard(epoch, e_loss, d_loss, total_loss):\n",
    "    writer.add_scalar('Loss/Encoder', e_loss, epoch)\n",
    "    writer.add_scalar('Loss/Decoder', d_loss, epoch)\n",
    "    writer.add_scalar('Loss/Total', total_loss, epoch)\n",
    "\n",
    "\n",
    "def log_gradients_to_tensorboard(model, epoch, model_name):\n",
    "    total_norm = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            norm = param.grad.norm(2).item()\n",
    "            total_norm += norm ** 2\n",
    "            writer.add_scalar(f'Gradients/{model_name}/{name}', norm, epoch)\n",
    "    total_norm = total_norm ** 0.5\n",
    "    writer.add_scalar(f'Gradients/{model_name}/total_norm', total_norm, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Variational Autoencoder (VAE) architecture can be visualized as follows:\n",
    "```\n",
    "    Input (x)\n",
    "       |\n",
    "       v\n",
    "   Encoder q(z|x)\n",
    "       |\n",
    "       v\n",
    "   Latent Space (z)\n",
    "       |\n",
    "       v\n",
    "   Decoder p(x|z)\n",
    "       |\n",
    "       v\n",
    "   Reconstructed (x̂)\n",
    "```\n",
    "We can see the key components of a VAE:\n",
    "\n",
    "- **Encoder:** We model $q(z|x)$ as a neural network with parameters $\\phi$. The network takes in an observation $x$ and outputs the parameters of a Gaussian distribution ie mean $\\mu_{\\phi}(x)$ and covariance $\\Sigma_{\\phi}(x)$.\n",
    "\n",
    "- **Decoder:** We model $p_{\\theta}(x|z)$ as a neural network with parameters $\\theta$. The network takes in a sampled latent variable $z$ from the distribution with parameters $\\mu_{\\phi}(x)$ and $\\Sigma_{\\phi}(x)$ and outputs a data sample $\\hat{x}$. Post training, we use the decoder to generate new data samples ie works as generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_dim = latent_dim  # Dimension of the latent space as instance variable because it an important hyperparameter.\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=4, stride=2, padding=1),  # Input: N x 3 x 128 x 128, Output: N x 32 x 64 x 64, 32 filters of size 4x4x3\n",
    "            nn.BatchNorm2d(32),  # Add BatchNorm\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),  # Input: N x 32 x 64 x 64, Output: N x 64 x 32 x 32, 64 filters of size 4x4x32\n",
    "            nn.BatchNorm2d(64),  # Add BatchNorm\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # Input: N x 64 x 32 x 32, Output: N x 128 x 16 x 16, 128 filters of size 4x4x64\n",
    "            nn.BatchNorm2d(128),  # Add BatchNorm\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # Input: N x 128 x 16 x 16, Output: N x 256 x 8 x 8, 256 filters of size 4x4x128\n",
    "            nn.BatchNorm2d(256),  # Add BatchNorm\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # Input: N x 256 x 8 x 8, Output: N x 512 x 4 x 4, 512 filters of size 4x4x256\n",
    "            nn.BatchNorm2d(512),  # Add BatchNorm\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc_mu = nn.Linear(512 * 4 * 4, latent_dim)  # Input: N x (512 * 4 * 4), Output: N x latent_dim, Fully connected layer for mean\n",
    "        self.fc_logvar = nn.Linear(512 * 4 * 4, latent_dim)  # Input: N x (512 * 4 * 4), Output: N x latent_dim, Fully connected layer for log variance becaues in vanilla VAE we use a diagonal covariance matrix ie we assume the latent variables are independent\n",
    "                                                             # It produces diagonal elements of the covariance matrix which are Log variance. During the reparameterization trick, we use these log variances to reconstruct the diagonal covariance matrix by exponentiating them.\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)  # Pass input through convolutional layers\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        mu = self.fc_mu(x)  # Calculate mean\n",
    "        logvar = self.fc_logvar(x)  # Calculate log variance\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Encoder(), input_size=(1, 3, IMG_SIZE, IMG_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim=LATENT_DIM):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_dim = latent_dim  # Dimension of the latent space\n",
    "\n",
    "        self.fc = nn.Linear(latent_dim, 512 * 4 * 4)  # Fully connected layer to transform latent vector to appropriate size for convolution\n",
    "\n",
    "        self.deconv_layers = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),  # Upsample to N x 512 x 8 x 8\n",
    "            nn.Conv2d(512, 256, kernel_size=3, stride=1, padding=1),  # Output: N x 256 x 8 x 8\n",
    "            nn.InstanceNorm2d(256),  # Add normalization layer\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),  # Upsample to N x 256 x 16 x 16\n",
    "            nn.Conv2d(256, 128, kernel_size=3, stride=1, padding=1),  # Output: N x 128 x 16 x 16\n",
    "            nn.InstanceNorm2d(128),  # Add normalization layer\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),  # Upsample to N x 128 x 32 x 32\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),  # Output: N x 64 x 32 x 32\n",
    "            nn.InstanceNorm2d(64),  # Add normalization layer\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),  # Upsample to N x 64 x 64 x 64\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),  # Output: N x 32 x 64 x 64\n",
    "            nn.InstanceNorm2d(32),  # Add normalization layer\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Upsample(scale_factor=2),  # Upsample to N x 32 x 128 x 128\n",
    "            nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1),  # Output: N x 3 x 128 x 128\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc(z)  # Transform latent vector to appropriate size for convolution\n",
    "        x = x.view(-1, 512, 4, 4)  # Reshape to 4D tensor for convolution\n",
    "        x = self.deconv_layers(x)  # Pass through convolutional layers\n",
    "        return x  # Return reconstructed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Decoder(), input_size=(1, LATENT_DIM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reparameterization Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to minimize ELBO:\n",
    "$$ F_{\\theta}(q) = \\mathbb{E}_{q(z|x)} \\left[ \\log p_{\\theta}(x|z) \\right] - D_{KL} \\left( q(z|x) \\mid p_{\\theta}(z) \\right) $$\n",
    "\n",
    "Focus on first term\n",
    "$$\\mathbb{E}_{q(z|x)} \\left[ \\log p_{\\theta}(x|z) \\right]$$\n",
    "\n",
    "We introduce a function $g_{\\phi}(\\epsilon)$ that transforms a noise variable $\\epsilon$ into $z$\n",
    "\n",
    "$$ z = g_{\\phi}(\\epsilon) $$\n",
    "\n",
    "This allows us to rewrite the expectation in terms of $\\epsilon$\n",
    "\n",
    "$$ \\mathbb{E}_{q(z|x)} \\left[ \\log p_{\\theta}(x|z) \\right] = \\mathbb{E}_{p(\\epsilon)} \\left[ \\log p_{\\theta}(x|g_{\\phi}(\\epsilon)) \\right] $$\n",
    "\n",
    "Where:\n",
    "- $\\epsilon \\sim p(\\epsilon)$ (typically a standard normal distribution)\n",
    "- $g_{\\phi}(\\epsilon)$ is our reparameterization function typically:\n",
    "$$ z = \\mu + \\sigma \\odot \\epsilon $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient can then be estimated using Monte Carlo sampling:\n",
    "\n",
    "$$ \\nabla_{\\phi} \\mathbb{E}_{q(z|x)}[\\log p_{\\theta}(x|z)] \\approx \\frac{1}{N} \\sum_{i=1}^N \\nabla_{\\phi} [\\log p_{\\theta}(x|g_{\\phi}(\\epsilon_i))]  \\quad \\epsilon_i \\sim p(\\epsilon) $$\n",
    "\n",
    "This approach allows the gradient to flow through the sampling process, enabling effective optimization of the VAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)  # Calculate standard deviation from log variance\n",
    "    eps = torch.randn_like(std).to(device)  # Generate random noise with same shape as std\n",
    "    z = mu + eps * std  # Reparameterization trick: z = μ + σ * ε\n",
    "    return z  # Return the sampled latent vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log likelihood is given by:\n",
    "\n",
    "$$ \\ell(\\theta) = \\log p_{\\theta}(x) $$\n",
    "\n",
    "We assume that each data point $x_i$ is associated with a latent variable $z_i$.\n",
    "Hence, we will introduce the latent variable $z$ and marginalize over it:\n",
    "\n",
    "$$ \\ell(\\theta) = \\log \\sum_z p_{\\theta}(x, z) $$\n",
    "\n",
    "Let $q(z|x)$ be a conditional distribution over $z$ given $x$.\n",
    "\n",
    "$$ \\ell(\\theta) = \\log \\sum_z q(z|x) \\frac{p_{\\theta}(x, z)}{q(z|x)} $$\n",
    "\n",
    "$$ \\ell(\\theta) = \\log \\mathbb{E}_{q(z|x)} \\left[ \\frac{p_{\\theta}(x, z)}{q(z|x)} \\right] $$\n",
    "\n",
    "By Jensen's inequality, we have:\n",
    "\n",
    "$$ \\log \\mathbb{E}_{q(z|x)} \\left[ \\frac{p_{\\theta}(x, z)}{q(z|x)} \\right] \\geq \\mathbb{E}_{q(z|x)} \\left[ \\log \\frac{p_{\\theta}(x, z)}{q(z|x)} \\right] $$\n",
    "\n",
    "Hence, we have:\n",
    "\n",
    "$$ \\ell(\\theta) \\geq \\mathbb{E}_{q(z|x)} \\left[ \\log \\frac{p_{\\theta}(x, z)}{q(z|x)} \\right] = F_{\\theta}(q) $$\n",
    "\n",
    "Where $F_{\\theta}(q)$ is the evidence lower bound (ELBO).\n",
    "\n",
    "$$ F_{\\theta}(q) = \\mathbb{E}_{q(z|x)} \\left[ \\log \\frac{p_{\\theta}(x|z)p_{\\theta}(z)}{q(z|x)} \\right] $$\n",
    "\n",
    "$$ F_{\\theta}(q) = \\mathbb{E}_{q(z|x)} \\left[ \\log p_{\\theta}(x|z) \\right] + \\mathbb{E}_{q(z|x)} \\left[ \\log \\frac{p_{\\theta}(z)}{q(z|x)} \\right] $$\n",
    "\n",
    "$$ F_{\\theta}(q) = \\mathbb{E}_{q(z|x)} \\left[ \\log p_{\\theta}(x|z) \\right] - \\mathbb{E}_{q(z|x)} \\left[ \\log \\frac{q(z|x)}{p_{\\theta}(z)} \\right] $$\n",
    "\n",
    "$$ F_{\\theta}(q) = \\mathbb{E}_{q(z|x)} \\left[ \\log p_{\\theta}(x|z) \\right] - D_{KL} \\left( q(z|x) \\mid p_{\\theta}(z) \\right) $$\n",
    "\n",
    "Here the first term is the conditional log likelihood of the data under the model. We want to maximise this term.\n",
    "\n",
    "The second term is the KL divergence between the posterior and the prior. We want to minimise this term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume $p_{\\theta}(x|z) = p_{\\theta}(x|g_{\\phi}(\\epsilon)) \\sim N(x; x_i, I)$, which is a model assumption. This allows us to calculate the log-likelihood $\\log p_{\\theta}(x|z)$ using the generated samples $\\hat{x}_j^i$ and the original input $x_i$ as follows(derivation skipped):\n",
    "\n",
    "$$ \\mathbb{E}_{q(x|z)} \\left[ \\log p_{\\theta}(x|z) \\right] = \\mathbb{E}_{p(\\epsilon)} \\left[ \\log p_{\\theta}(x|g_{\\phi}(\\epsilon)) \\right] \\approx \\frac{1}{m} \\sum_{j=1}^m \\log p_{\\theta}(x|g_{\\phi}(\\epsilon_j)) \\propto \\frac{1}{m} \\sum_{j=1}^m \\|x_i - \\hat{x}_j^i\\|_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruction loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(x_reconstructed, x):\n",
    "    \"\"\"\n",
    "    Calculates the reconstruction loss using Mean Squared Error.\n",
    "    \n",
    "    Args:\n",
    "    x_reconstructed (torch.Tensor): The reconstructed input from the decoder with shape (batch_size, input_dimension).\n",
    "    x (torch.Tensor): The original input with shape (batch_size, input_dimension).\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The reconstruction loss, a scalar value representing the total loss across the batch.\n",
    "    \"\"\"\n",
    "    return nn.functional.mse_loss(x_reconstructed, x, reduction='sum')  # Calculate reconstruction loss using Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KL Divergence loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that:\n",
    "\n",
    "$$ F_{\\theta}(q) = \\mathbb{E}_{q(z|x)} \\left[ \\log p_{\\theta}(x|z) \\right] - D_{KL} \\left( q(z|x) \\mid p_{\\theta}(z) \\right) $$\n",
    "\n",
    "the second term is:\n",
    "\n",
    "$$ D_{KL} \\left( q(z|x) \\mid p_{\\theta}(z) \\right) $$\n",
    "\n",
    "We want to minimise this term.\n",
    "\n",
    "We assume the latent prior $p_\\theta(z) \\sim N(0, I)$, where $I$ is the identity matrix.\n",
    "\n",
    "The approximate posterior $q(z|x)$ is modeled as $N(z; \\mu_\\phi(x), \\Sigma_\\phi(x))$.\n",
    "\n",
    "Given these assumptions, we can derive the KL divergence in closed form as:\n",
    "\n",
    "$$ D_{KL}(N(z; \\mu_\\phi(x), \\Sigma_\\phi(x)) \\| N(0, I)) = \\frac{1}{2} \\sum_{j=1}^J \\left( \\mu_{\\phi,j}^2(x) + \\Sigma_{\\phi,j}(x) - \\log \\Sigma_{\\phi,j}(x) - 1 \\right) $$\n",
    "\n",
    "Where:\n",
    "- $J$ is the dimensionality of the latent space\n",
    "- $\\mu_{\\phi,j}(x)$ is the j-th element of the mean vector\n",
    "- $\\Sigma_{\\phi,j}(x)$ is the j-th diagonal element of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_loss(mu, logvar):\n",
    "    \"\"\"\n",
    "    Calculates the KL divergence loss.\n",
    "    \n",
    "    Args:\n",
    "    mu (torch.Tensor): The mean of the latent distribution.\n",
    "    logvar (torch.Tensor): The log variance of the latent distribution.\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: The KL divergence loss.\n",
    "    \"\"\"\n",
    "    kl_divergence_loss_value = 0.5 * torch.sum(mu.pow(2) + logvar.exp() - logvar - 1) \n",
    "    return kl_divergence_loss_value  # Return the calculated KL divergence loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        # Directory containing all images (including subfolders)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform  # Transformations to apply to images\n",
    "        self.image_files = []  # List to store all image file paths\n",
    "\n",
    "        # Traverse through all subfolders\n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    self.image_files.append(os.path.join(root, file))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)  # Return the total number of images\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]  # Get image path\n",
    "        image = Image.open(img_path).convert(\n",
    "            'RGB')  # Open image and convert to RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations if any\n",
    "\n",
    "        return image, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),          # Randomly flip images horizontally\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors and scale to [0, 1]\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from the specified relative path\n",
    "animal_dataset = datasets.ImageFolder(root='data/Animals_data/animals/animals',  # Specify the root directory of the dataset\n",
    "                                       transform=transform)  # Apply the defined transformations to the dataset\n",
    "\n",
    "butterly_dataset = ImageDataset(root_dir=\"data/butterfly_data\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader for the dataset to enable batch processing\n",
    "d = butterly_dataset if dataset == BUTTERFLY else animal_dataset\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    d, BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights_decoder(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # Check if this Conv layer is in decoder (following Upsample)\n",
    "        # Decoder conv layers need slightly larger weights to handle upsampled features\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.002)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0.001)\n",
    "            \n",
    "    elif isinstance(m, nn.Linear):\n",
    "        # Decoder linear layers: slightly larger initialization\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.002)\n",
    "        torch.nn.init.constant_(m.bias, 0.001)\n",
    "\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n",
    "        if m.weight is not None:\n",
    "            # Slightly larger scale for decoder norm layers to handle upsampled features\n",
    "            torch.nn.init.constant_(m.weight, 0.2)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "            \n",
    "def init_weights_encoder(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        # Encoder conv layers: keep very small random initialization\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=0.001)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "            \n",
    "    elif isinstance(m, nn.Linear):\n",
    "        if 'mu' in m.__class__.__name__.lower():\n",
    "            # Initialize mu layer to produce values very close to 0\n",
    "            torch.nn.init.zeros_(m.weight)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "            # Add tiny noise to prevent pure zeros\n",
    "            with torch.no_grad():\n",
    "                m.weight.data += torch.randn_like(m.weight) * 0.0001\n",
    "                \n",
    "        elif 'logvar' in m.__class__.__name__.lower():\n",
    "            # Initialize logvar layer to produce exact zeros\n",
    "            torch.nn.init.zeros_(m.weight)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "            \n",
    "        else:\n",
    "            # Regular linear layers get very small random initialization\n",
    "            torch.nn.init.normal_(m.weight, mean=0.0, std=0.001)\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "            \n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n",
    "        if m.weight is not None:\n",
    "            torch.nn.init.constant_(m.weight, 0.1)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(encoder_model: Encoder, decoder_model: Decoder, epoch_index, img_dir, display=False):\n",
    "    # Generate and save images for the current epoch\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        decoder_model.eval()  # Set decoder to evaluation mode\n",
    "        encoder_model.eval()  # Set encoder to evaluation mode\n",
    "\n",
    "        # Generate random latent vectors for new images\n",
    "        sample_latent_vector = torch.randn(100, LATENT_DIM).to(device)  # Generate random latent vectors\n",
    "        generated_images = decoder_model(sample_latent_vector)  # Generate images using the decoder\n",
    "\n",
    "        # Collect real images from the dataloader until we have exactly 100\n",
    "        real_images_list = []\n",
    "        for images, _ in dataloader:\n",
    "            real_images_list.append(images.to(device))\n",
    "            if sum(img.size(0) for img in real_images_list) >= 100:\n",
    "                break\n",
    "\n",
    "        # Concatenate all batches into a single tensor and slice to get exactly 100\n",
    "        real_images = torch.cat(real_images_list, dim=0)[:100]\n",
    "\n",
    "        # Encode real images to get latent vectors\n",
    "        mean, log_var = encoder_model(real_images)\n",
    "        encoded_latents = reparameterize(mean, log_var).to(device)\n",
    "        reconstructed_images = decoder_model(encoded_latents)  # Reconstruct the real images\n",
    "\n",
    "        decoder_model.train()  # Set decoder back to training mode\n",
    "        encoder_model.train()  # Set encoder back to training mode\n",
    "\n",
    "    if display:\n",
    "        # Function to display grid using matplotlib\n",
    "        def show_images(tensor, title):\n",
    "            tensor = (tensor + 1) / 2.0\n",
    "            grid = make_grid(tensor, nrow=10, padding=2, normalize=True)  # Create grid\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(grid.permute(1, 2, 0).cpu().numpy())  # Convert to numpy for plotting\n",
    "            plt.axis('off')\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "\n",
    "        show_images(generated_images, f'Generated Images - Epoch {epoch_index + 1}')\n",
    "        show_images(reconstructed_images, f'Reconstructed Images - Epoch {epoch_index + 1}')\n",
    "    else:\n",
    "\n",
    "        # Save generated images\n",
    "        image_path_gen = f'{img_dir}/generated_{epoch_index + 1}.png'\n",
    "        save_image(make_grid(generated_images, nrow=10, padding=2, normalize=True), image_path_gen)\n",
    "\n",
    "        # Save reconstructed images\n",
    "        image_path_recon = f'{img_dir}/reconstructed_{epoch_index + 1}.png'\n",
    "        save_image(make_grid(reconstructed_images, nrow=10, padding=2, normalize=True), image_path_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papermill_description=Q1\n",
    "\n",
    "def train_vae(beta=1, img_dir=IMG_DIR):\n",
    "    encoder_model = Encoder().to(device)\n",
    "    decoder_model = Decoder().to(device)\n",
    "    encoder_model.apply(init_weights_encoder)\n",
    "    decoder_model.apply(init_weights_decoder)\n",
    "    encoder_optimizer = torch.optim.Adam(encoder_model.parameters(), lr=lr)\n",
    "    decoder_optimizer = torch.optim.Adam(decoder_model.parameters(), lr=lr)\n",
    "\n",
    "    encoder_scheduler = optim.lr_scheduler.StepLR(\n",
    "        encoder_optimizer, step_size=NUM_EPOCHS//3, gamma=0.5)\n",
    "    decoder_scheduler = optim.lr_scheduler.StepLR(\n",
    "        decoder_optimizer, step_size=NUM_EPOCHS//3, gamma=0.5)\n",
    "\n",
    "    # Training loop\n",
    "    # Iterate through the specified number of epochs\n",
    "    for epoch_index in range(NUM_EPOCHS):\n",
    "\n",
    "        # Iterate through batches of images\n",
    "        for images_batch, _ in dataloader:\n",
    "            # Move the batch of images to the specified device (GPU/CPU)\n",
    "            images_batch = images_batch.to(device)\n",
    "\n",
    "            # Zero gradients for both encoder and decoder optimizers\n",
    "            encoder_optimizer.zero_grad()  # Reset gradients for the encoder optimizer\n",
    "            decoder_optimizer.zero_grad()  # Reset gradients for the decoder optimizer\n",
    "\n",
    "            # Forward pass through encoder to obtain mean and log variance\n",
    "            # Encode the input images to get mean and log variance\n",
    "            mean_output, log_variance_output = encoder_model(images_batch)\n",
    "\n",
    "            latent_vector_z = reparameterize(\n",
    "                mean_output, log_variance_output)\n",
    "            reconstructed_samples_list = []  # List to store multiple reconstructed samples\n",
    "            for sample_index in range(NUM_SAMPLES):\n",
    "                # Sample latent vectors using the reparameterization trick and move to device\n",
    "                latent_vector_z = reparameterize(\n",
    "                    mean_output, log_variance_output).to(device)\n",
    "                # Decode the sampled latent vectors to reconstruct images\n",
    "                reconstructed_sample = decoder_model(latent_vector_z)\n",
    "                # Add the reconstructed sample to the list\n",
    "                reconstructed_samples_list.append(reconstructed_sample)\n",
    "            reconstructed_samples_tensor = torch.stack(reconstructed_samples_list, dim=0).to(\n",
    "                device)  # Stack all reconstructed samples into a single tensor and move to device\n",
    "\n",
    "            # Compute mean reconstruction across all samples\n",
    "            mean_reconstructed_images = torch.mean(reconstructed_samples_tensor, dim=0).to(\n",
    "                device)  # Calculate the mean of all reconstructed samples and move to device\n",
    "\n",
    "            # Calculate reconstruction loss using the mean reconstructed images and the original images\n",
    "            # Compute reconstruction loss (both tensors are already on device)\n",
    "            reconstruction_loss_value = reconstruction_loss(\n",
    "                mean_reconstructed_images, images_batch)\n",
    "\n",
    "            # Calculate KL divergence loss using the mean and log variance from the encoder\n",
    "            # Compute KL divergence loss (both tensors are already on device)\n",
    "            kl_divergence_loss_value = kl_divergence_loss(\n",
    "                mean_output, log_variance_output)\n",
    "\n",
    "            # Compute total loss by summing reconstruction loss and KL divergence loss\n",
    "            total_loss_value = reconstruction_loss_value + \\\n",
    "                beta*kl_divergence_loss_value  # Total loss for both encoder and decoder\n",
    "\n",
    "            # Backward pass for both encoder and decoder (using total loss)\n",
    "            total_loss_value.backward()  # Compute gradients for both encoder and decoder\n",
    "\n",
    "            encoder_optimizer.step()  # Update the encoder's parameters based on the gradients\n",
    "            decoder_optimizer.step()  # Update the decoder's parameters based on the gradients\n",
    "\n",
    "        encoder_scheduler.step()\n",
    "        decoder_scheduler.step()\n",
    "        log_gradients_to_tensorboard(encoder_model, epoch_index, 'Encoder')\n",
    "        log_gradients_to_tensorboard(decoder_model, epoch_index, 'Decoder')\n",
    "\n",
    "        if epoch_index % 5 == 0:\n",
    "            print(f\"Epoch [{epoch_index}/{NUM_EPOCHS}]  Encoder loss: {\n",
    "                  kl_divergence_loss_value.item():.4f}, Decoder loss: {reconstruction_loss_value.item():.4f}\")\n",
    "            generate_and_save_images(encoder_model, decoder_model, epoch_index, img_dir)\n",
    "\n",
    "        log_losses_to_tensorboard(\n",
    "            epoch_index, kl_divergence_loss_value.item(), reconstruction_loss_value.item(), total_loss_value.item())\n",
    "\n",
    "    # Print a message indicating that the entire training process has finished\n",
    "    print(\"Training completed.\")\n",
    "    return encoder_model, decoder_model\n",
    "\n",
    "\n",
    "\n",
    "encoder_model_path = f'VAE/models/encoder_{dataset}_ns_{NUM_SAMPLES}{tag}.pth'\n",
    "decoder_model_path = f'VAE/models/decoder_{dataset}_ns_{NUM_SAMPLES}{tag}.pth'\n",
    "if TRAIN_VAE:\n",
    "    encoder_model, decoder_model = train_vae()\n",
    "    torch.save(encoder_model.state_dict(), encoder_model_path)\n",
    "    torch.save(decoder_model.state_dict(), decoder_model_path)\n",
    "    print(f\"Models saved in 'VAE/models' folder\")\n",
    "else:\n",
    "    encoder_model = Encoder().to(device)\n",
    "    decoder_model = Decoder().to(device)\n",
    "    encoder_model.load_state_dict(torch.load(encoder_model_path, weights_only=True))\n",
    "    decoder_model.load_state_dict(torch.load(decoder_model_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_images(encoder_model, decoder_model,\n",
    "                         NUM_EPOCHS, IMG_DIR, display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot lost curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_curves():\n",
    "    event_acc.Reload()\n",
    "\n",
    "    # Extract the scalar values for KL Divergence Loss, Reconstruction Loss, and Total Loss\n",
    "    kl_loss_values = event_acc.Scalars('Loss/Encoder')  # exact tag for KL divergence loss\n",
    "    reconstruction_loss_values = event_acc.Scalars('Loss/Decoder')  # exact tag for reconstruction loss\n",
    "    total_loss_values = event_acc.Scalars('Loss/Total')  # exact tag for total loss\n",
    "\n",
    "    # Extract steps and values for each scalar\n",
    "    kl_steps = [scalar.step for scalar in kl_loss_values]\n",
    "    kl_losses = [scalar.value for scalar in kl_loss_values]\n",
    "\n",
    "    reconstruction_steps = [scalar.step for scalar in reconstruction_loss_values]\n",
    "    reconstruction_losses = [scalar.value for scalar in reconstruction_loss_values]\n",
    "\n",
    "    total_steps = [scalar.step for scalar in total_loss_values]\n",
    "    total_losses = [scalar.value for scalar in total_loss_values]\n",
    "\n",
    "    # Plot using Matplotlib\n",
    "    plt.figure(figsize=(10,6))\n",
    "\n",
    "    # Plot KL Divergence Loss\n",
    "    plt.plot(kl_steps, kl_losses, label='KL Divergence Loss', color='blue')\n",
    "\n",
    "    # Plot Reconstruction Loss\n",
    "    plt.plot(reconstruction_steps, reconstruction_losses, label='Reconstruction Loss', color='green')\n",
    "\n",
    "    # Plot Total Loss\n",
    "    plt.plot(total_steps, total_losses, label='Total Loss', color='red')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Components Over Time')\n",
    "\n",
    "    # Add legend and grid\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "if TRAIN_VAE:\n",
    "    plot_curves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Perform posterior inference for a pair of images and plot the generated images in the path of linearly interpolated latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# papermill_description=Q5\n",
    "\n",
    "def encode_images(encoder, images):\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        mu, log_var = encoder(images.to(device))\n",
    "    return mu, log_var\n",
    "\n",
    "def interpolate_latents(z1, z2, num_points=10):\n",
    "    alphas = np.linspace(0, 1, num_points)\n",
    "    interpolated = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        z = (1 - alpha) * z1 + alpha * z2\n",
    "        interpolated.append(z)\n",
    "    \n",
    "    return torch.stack(interpolated)\n",
    "\n",
    "def decode_interpolations(decoder, interpolated_latents):\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed = decoder(interpolated_latents.to(device))\n",
    "    return reconstructed\n",
    "\n",
    "def plot_interpolation_path(original_images, reconstructed_images, pair_idx, save_path=None):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    \n",
    "    # Plot original pair\n",
    "    plt.subplot(1, 12, 1)\n",
    "    plt.imshow(original_images[0].cpu().permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.title('Original 1')\n",
    "    \n",
    "    # Plot interpolations\n",
    "    for i in range(10):\n",
    "        plt.subplot(1, 12, i + 2)\n",
    "        plt.imshow(reconstructed_images[i].cpu().permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Step {i+1}')\n",
    "    \n",
    "    # Plot second original\n",
    "    plt.subplot(1, 12, 12)\n",
    "    plt.imshow(original_images[1].cpu().permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.title('Original 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(f'{save_path}/interpolation_pair_{pair_idx}.png')\n",
    "    plt.show()\n",
    "\n",
    "def perform_interpolation_analysis(encoder, decoder, dataset, num_pairs=10, num_points=10,  save_path=None):\n",
    "    \n",
    "    for pair_idx in range(num_pairs):\n",
    "        # Randomly select two images\n",
    "        idx1, idx2 = np.random.choice(len(dataset), 2, replace=False)\n",
    "        img1, img2 = dataset[idx1][0], dataset[idx2][0]  # Assuming dataset returns (image, label) pairs\n",
    "        \n",
    "        # Add batch dimension\n",
    "        images = torch.stack([img1, img2])\n",
    "        \n",
    "        # Encode images to get latent representations\n",
    "        mu, log_var = encode_images(encoder, images)\n",
    "        \n",
    "        # Sample from the posterior using reparameterization trick\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        \n",
    "        # Interpolate between the two latent vectors\n",
    "        interpolated = interpolate_latents(z[0], z[1], num_points)\n",
    "        interpolated = (interpolated + 1) / 2.0\n",
    "\n",
    "        \n",
    "        # Decode interpolated vectors\n",
    "        reconstructed = decode_interpolations(decoder, interpolated)\n",
    "        \n",
    "        # Plot results\n",
    "        plot_interpolation_path([img1, img2], reconstructed, pair_idx, save_path)\n",
    "        \n",
    "if LOAD_BETA_VAE:\n",
    "    perform_interpolation_analysis(beta_encoder_model, beta_decoder_model, butterly_dataset, save_path=\"VAE/plots\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
